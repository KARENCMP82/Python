{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KARENCMP82/Python/blob/main/1_METRICAS_DISTANCIAS_19MAR25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Nuclio Digital School - Máster en Data Science**\n",
        "\n",
        "# Unsupervised Learning: Similitud y distancias\n",
        "\n",
        "# *Profesora: Raquel Revilla*\n",
        "\n"
      ],
      "metadata": {
        "id": "7Nck9vp-0d1N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE8nAuYHt1VW"
      },
      "source": [
        "<a id = \"objetivos\"></a>\n",
        "# Objetivos del Notebook\n",
        "[Volver al índice](#table_of_contents)\n",
        "\n",
        "El presente notebook va a introducir al alumno a los conceptos básicos de **Aprendizaje No Supervisado (UL)**.\n",
        "\n",
        "***¿Por qué decimos no supervisado?***\n",
        "\n",
        "Tal y como su nombre indica, en el **Aprendizaje No Supervisado** tenemos un conjunto de datos de entrenamiento sin el correspondiente valor del target. <u>**Tenemos la X pero no la Y.**</u> Al principio podría parecer que si no tenemos el target de entrenamiento el dataset carece de valor, pero esto no es así.\n",
        "\n",
        "Entre los principales problemas que buscan resolver los algoritmos de **Aprendizaje No Supervisado** se encuentran:\n",
        "\n",
        "***¿Qué entendéis por clustering?***\n",
        "\n",
        "1. **Clusterización/Segmentación de los datos**: se buscan grupos de <u>**ejemplos similares**</u> entre si y diferentes al resto de grupos. Por ejemplo: tenemos un dataset de clientes y queremos ver que grupos existen en función de determinados patrones que están ocultos en los datos. Ejecutando nuestro algoritmo de segmentación podríamos obtener las siguientes etiquetas: clientes digitales, clientes eco, clientes que compran exclusivamente en tiendas físicas, etc. Como es lógico, los clientes del grupo de \"digitales\" son muy parecidos entre sí y muy diferentes a los clientes \"físicos\".\n",
        "\n",
        "***¿Qué es un recomendador?***\n",
        "\n",
        "2. **Collaborative Filtering (Recommenders)**: con estos algoritmos queremos determinar <u>**la similitud**</u> entre ejemplos con el objetivo de ponderar alguna métrica para realizar recomendaciones. Se trata de ofrecer a clientes similares productos que vayan a gustar. Por ejemplo: tenemos un dataset de reviews de péliculas (n clientes que han hecho reviews a x péliculas). Ejecutando nuestro algoritmo de recomendación obtendriamos un score de similitud para cada cliente contra el resto de clientes. Sabríamos que un cliente se parece a otro porque han puesto notas similares a las mismas péliculas y con esta información podríamos construir una oferta personalizada de nuevas péliculas.\n",
        "\n",
        "\n",
        "3. **Reducción de la dimensionalidad**: busca pasar de un espacio de **\"m\"** dimensiones a otro espacio de **\"n\"** dimensiones de tal manera que n << m. La utilidad de reducir la dimensionalidad de un dataset es para conseguir que sea mucho más manejable. Un dataset \"pequeño\" es más fácil de analizar, visualizar etc. Por este motivo, la reducción de la dimensionalidad suele ser un paso concreto dentro de un proyecto de Machine Learning (no un fin en si mismo).\n",
        "\n",
        "![UL_INTRO](https://drive.google.com/uc?export=view&id=1CrALL3vdJxpY-MOWqLc-je92XjIJcyLj)\n",
        "\n",
        "Un concepto recurrente que aparece en los puntos anteriores es **la idea de similitud.** Para agrupar a clientes similares (clústerización) o bien hacer recomendaciones basadas en gustos similares (collaborative filtering), **necesito poder calcular una métrica que me permita comparar a los clientes entre si de una manera objetiva.**\n",
        "\n",
        "En este notebook veremos algunas de las formas más comunes para hacerlo.\n",
        "\n",
        "Al final de la sesión, el alumno se debe sentir cómodo con los siguientes conceptos:\n",
        "\n",
        "1. Comprender que es la distancia Euclídea y la de Manhattan (**CORE IDEA**).\n",
        "\n",
        "2. **Aplicación de estas dos distancias a un dataset**, para encontrar en base a nuestras variables a los clientes más parecidos (los que más cerca están en nuestro espacio dimensional).\n",
        "\n",
        "3. Ver en la práctica que problemas pueden surgir si tenemos diferentes escalas en nuestro dataset (**CORE IDEA**).\n",
        "\n",
        "4. Aplicar la idea de distancias para ver algoritmos de \"aprendizaje por instancias\".\n",
        "\n",
        "---\n",
        "\n",
        "Las ideas más importantes están marcadas con **CORE IDEA** y recomendamos a los alumnos que centren sus esfuerzos en estos apartados.\n",
        "\n",
        "Al final del notebook, hay un sección de referencias y lecturas recomendables para que el alumno pueda seguir profundizando en estos conceptos.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-13T15:05:35.830483Z",
          "start_time": "2021-11-13T15:05:35.802558Z"
        },
        "id": "XN5PEgETt1VY"
      },
      "source": [
        "<a id = \"table_of_contents\"></a>\n",
        "\n",
        "# Índice\n",
        "\n",
        "[Importación de las Principales Librerías](#import_modules)\n",
        "\n",
        "[GLOBAL_VARIABLES](#global_variables)\n",
        "\n",
        "[Funciones Auxiliares](#helpers)\n",
        "\n",
        "[Generación de un dataset](#dataset)\n",
        "\n",
        "[Cálculo de distancias](#distances)\n",
        "\n",
        "---> [Distancia Euclídea Sin Estandarización](#euclidea_bad)\n",
        "\n",
        "---> [Distancia de Manhattan Sin Estandarización](#manhattan_bad)\n",
        "\n",
        "---> [Distancia Euclídea Con Estandarización](#euclidea_bien)\n",
        "\n",
        "---> [Distancia de Manhattan Con Estandarización](#manhattan_bien)\n",
        "\n",
        "[Otras formas de procesar los datos](#transformers)\n",
        "\n",
        "[Comparación de los Transformers](#scaler_comparison)\n",
        "\n",
        "[Aprendizaje Basado en Instancias](#knn)\n",
        "\n",
        "[Conclusión](#conclusión)\n",
        "\n",
        "[Referencias y lecturas recomendables](#referencias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqL2Z9KKt1VZ"
      },
      "source": [
        "<a id = \"import_modules\"></a>\n",
        "# Importación de las Principales Librerías\n",
        "[Volver al índice](#table_of_contents)\n",
        "\n",
        "En esta sección hacemos los imports de las principales librerías."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-14T15:53:56.088191Z",
          "start_time": "2021-11-14T15:53:54.350502Z"
        },
        "id": "kCTsSlFet1VZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn import set_config\n",
        "set_config(transform_output = \"pandas\")\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.metrics.pairwise import manhattan_distances, euclidean_distances\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.impute import KNNImputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kD94o3vvt1Va"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tPZv_lnt1Vb"
      },
      "source": [
        "<a id = \"global_variables\"></a>\n",
        "# GLOBAL_VARIABLES\n",
        "[Volver al índice](#table_of_contents)\n",
        "\n",
        "Definimos las GLOBAL_VARIABLES que afectarán a nuestro notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-14T15:53:56.103411Z",
          "start_time": "2021-11-14T15:53:56.088191Z"
        },
        "id": "RZUruJNSt1Vb"
      },
      "outputs": [],
      "source": [
        "COLOR = \"#2a9d8f\"\n",
        "COLOR_MAX = \"#D8E4FF\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwOHV_7tt1Vb"
      },
      "source": [
        "<a id = \"helpers\"></a>\n",
        "# Funciones Auxiliares\n",
        "[Volver al índice](#table_of_contents)\n",
        "\n",
        "Definimos las funciones que vamos a usar en el resto del notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-14T15:53:56.210332Z",
          "start_time": "2021-11-14T15:53:56.106404Z"
        },
        "id": "OJcKkow7t1Vb"
      },
      "outputs": [],
      "source": [
        "def plot_euclidean_distance():\n",
        "    '''\n",
        "    Makes a simple plot of an Euclidean distances in a 2D space.\n",
        "    Returns nothing. Renders the plot on the function call.\n",
        "    '''\n",
        "    # instanciamos la figure y el axes\n",
        "    fig = plt.figure(figsize = (10, 10))\n",
        "    ax = fig.add_subplot(111)\n",
        "\n",
        "    # creamos una espacio cartesiona\n",
        "    lims = (-10, 10)\n",
        "    alpha = 0.5\n",
        "    ax.set_xlim(lims)\n",
        "    ax.set_ylim(lims)\n",
        "\n",
        "    # pintamos lineas horizontales y verticales\n",
        "    ax.vlines(0, -10, 10, linestyles = \"--\", alpha = alpha)\n",
        "    ax.hlines(0, -10, 10, linestyles = \"--\", alpha = alpha)\n",
        "\n",
        "    # pintamos los dos catetos y la hipotenusa\n",
        "    ax.plot([0, 5], [0, 5], alpha = alpha, color = \"red\", lw = 2) # hipotenusa\n",
        "    ax.text(2, 3.5, s = \"d\", fontsize = 8, color = \"red\")\n",
        "\n",
        "    ax.plot([0, 5], [0, 0], alpha = alpha, color = \"blue\", lw = 2) # cateto azul\n",
        "    ax.text(1.5, -1, s = \"x2 - x1\", fontsize = 8, color = \"blue\")\n",
        "\n",
        "    ax.plot([5, 5], [5, 0], alpha = alpha, color = \"green\", lw = 2) # cateto verde\n",
        "\n",
        "    # origen\n",
        "    ax.scatter([0, 0], [0, 0], alpha = alpha, color = \"black\", lw = 2)\n",
        "    ax.text(-1, -1, s = \"(x1, y1)\", fontsize = 8, color = \"black\")\n",
        "\n",
        "    # punto 2\n",
        "    ax.scatter([0, 5], [0, 5], alpha = alpha, color = \"black\", lw = 2)\n",
        "    ax.text(5.5, 5.5, s = \"(x2, y2)\", fontsize = 8, color = \"black\")\n",
        "\n",
        "    # punto en el eje x\n",
        "    ax.scatter([5, 0], [0, 0], alpha = alpha, color = \"black\", lw = 2)\n",
        "\n",
        "    # cálculo\n",
        "    ax.text(1.5, -1, s = \"x2 - x1\", fontsize = 8, color = \"blue\")\n",
        "    ax.text(5.5, 2, s = \"y2 - y1\", fontsize = 8, color = \"green\")\n",
        "\n",
        "    # creamos las listas para los ejes (-10 hasta 10)\n",
        "    x_ticks = [x for x in range(-10, 11)]\n",
        "    y_ticks = [y for y in range(-10, 11)]\n",
        "\n",
        "    # enumeramos los ejes desde -10 hasta 10\n",
        "    ax.set_xticks(x_ticks)\n",
        "    ax.set_yticks(y_ticks)\n",
        "\n",
        "    # ponemos el título\n",
        "    fig.suptitle(\"Euclidean Distance Between 2 Vectors in a 2D Space\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-14T15:53:56.226290Z",
          "start_time": "2021-11-14T15:53:56.212328Z"
        },
        "id": "M6qbXdmft1Vc"
      },
      "outputs": [],
      "source": [
        "def calculate_distances(X, index, distance_func):\n",
        "    '''\n",
        "    Calculates the distances between vectors with the specified function you pass.\n",
        "    Returns a pandas DataFrame.\n",
        "    '''\n",
        "    distances = distance_func(X = X)\n",
        "    distances = pd.DataFrame(distances, index = index, columns = index)\n",
        "    distances = round(distances, 1)\n",
        "    distances.replace(to_replace = 0, value = np.nan, inplace = True)\n",
        "\n",
        "    return distances"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-14T15:53:56.241674Z",
          "start_time": "2021-11-14T15:53:56.229282Z"
        },
        "id": "b7Jh5-V1t1Vc"
      },
      "outputs": [],
      "source": [
        "def format_cell_based_on_target_value(value, target_value, highlight = 'background-color: yellow'):\n",
        "    '''\n",
        "    Formats a cell based on a target_value.\n",
        "    Returns a background color or pass.\n",
        "    '''\n",
        "    if value == None:\n",
        "        pass\n",
        "    elif value == target_value:\n",
        "        return highlight\n",
        "    else:\n",
        "        return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GSJKSawt1Vc"
      },
      "source": [
        "<a id = \"dataset\"></a>\n",
        "# Generación de un dataset\n",
        "[Volver al índice](#table_of_contents)\n",
        "\n",
        "Vamos a generar un dataset dummy con el que vamos a trabajar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-14T15:53:56.273590Z",
          "start_time": "2021-11-14T15:53:56.259627Z"
        },
        "id": "ZC1-pCiwt1Vc"
      },
      "outputs": [],
      "source": [
        "data = {\n",
        "    \"1. Edad\":[18, 30, 21, 25, 33, 33],\n",
        "    \"2. Peso\":[90, 70, 77, 60, 80, 80],\n",
        "    \"3. Altura\":[180, 175, 170, 183, 185, 185],\n",
        "    \"4. Nómina\":[2000, 1500, 3000, 1800, 900, 3000]\n",
        "}\n",
        "\n",
        "index = [\"Cliente1\", \"Cliente2\", \"Cliente3\", \"Cliente4\", \"Cliente5\", \"Cliente6\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd3udO5lt1Vd"
      },
      "source": [
        "Vamos a comenzar la sesión con una pregunta sencilla.\n",
        "\n",
        "### Pregunta 1: ¿el Cliente6 a que cliente se le parece más?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-14T15:53:56.304506Z",
          "start_time": "2021-11-14T15:53:56.275587Z"
        },
        "id": "RihotGowt1Vd"
      },
      "outputs": [],
      "source": [
        "# NOTA IMPORTANTE:\n",
        "# Hacemos el transpose para visualizar más fácil el dataset (cada cliente está en 1 columna)\n",
        "\n",
        "# El alumno debe tener presente que\n",
        "# el cálculo de las distancias se realizan a nivel de filas (cliente1 vs cliente2, cliente1 vs cliente3 etc...)\n",
        "\n",
        "df_clientes = pd.DataFrame(data = data, index = index)\n",
        "df_clientes.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7dwMBknt1Vd"
      },
      "outputs": [],
      "source": [
        "df_clientes[[\"1. Edad\"]].T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_l42-i1t1Vd"
      },
      "source": [
        "<a id = \"distances\"></a>\n",
        "# Cálculo de distancias\n",
        "[Volver al índice](#table_of_contents)\n",
        "\n",
        "En el dataset anterior tenemos un conjunto de 6 clientes (6 instancias) y tengo recogidos un total de 4 variables para cada uno de ellos. Tengo representados a mis clientes en 4 dimensiones (Edad, Peso, Altura y Nómina).\n",
        "\n",
        "Una idea muy sencilla e intuitiva para ver que clientes son más similares entre si es utilizar la noción de \"proximidad\". Por ejemplo: si sólo tuviera 1 dimensión (la edad), sería muy fácil concluir que Cliente6 y Cliente5 son los mas parecidos porque los dos tienen la misma edad. La distancia que les separa (en edad) es cero.\n",
        "\n",
        "Veamos ahora como puedo generalizar esta idea a n dimensiones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGmLnvy4t1Ve"
      },
      "source": [
        "<a id = \"euclidea_bad\"></a>\n",
        "# --> Similitud basada en la distancia Euclídea (dataset sin estandarizar)\n",
        "[Volver al índice](#table_of_contents)\n",
        "\n",
        "La [distancia Euclídea](https://es.wikipedia.org/wiki/Distancia_euclidiana) es la más utilizada dentro del mundo de **Aprendizaje No Supervisado**.\n",
        "\n",
        "Desde un punto de vista matemático, es la distancia ordinaria entre dos puntos en un espacio euclídeo. Esto implica que el camino más corto entre 2 puntos es **\"ir por la diagonal\"**.\n",
        "\n",
        "Se puede calcular fácilmente utilizando el teorema de Pitágoras (calculamos la hipotenusa a partir de los catetos de una triángulo).\n",
        "\n",
        "Viene dada por la siguiente expresión matemática (para 2 dimensiones):\n",
        "\n",
        "![Distancia_Euclídea](https://drive.google.com/uc?export=view&id=1-gQz_021Z_Dp_IDm2NIvhEu9ZdIAhvv4)\n",
        "\n",
        "Se puede generalizar hasta N dimensiones con la siguiente fórmula:\n",
        "\n",
        "![Distancia_Euclídea_General](https://drive.google.com/uc?export=view&id=1cbAXVW1c9CpZqMBcFe3nJTeVCJUKh4XH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoAD6v_tt1Ve"
      },
      "outputs": [],
      "source": [
        "plot_euclidean_distance()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kNPp78It1Ve"
      },
      "source": [
        "Vamos a calcular la distancia euclídea para 2 clientes cualquiera y posteriormente lo haremos para todas las instancias de nuestro dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-14T15:53:56.664669Z",
          "start_time": "2021-11-14T15:53:56.604778Z"
        },
        "id": "huTPS1Fbt1Ve"
      },
      "outputs": [],
      "source": [
        "df_cl1_2 = df_clientes.T.iloc[:, :2]\n",
        "df_cl1_2[\"Diff\"] = df_cl1_2[\"Cliente1\"] - df_cl1_2[\"Cliente2\"]\n",
        "df_cl1_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-14T15:53:56.695585Z",
          "start_time": "2021-11-14T15:53:56.670651Z"
        },
        "id": "YojNkcivt1Ve"
      },
      "outputs": [],
      "source": [
        "eucl_cl1_cl2 = (((18 - 30) ** 2) + ((90 - 70) ** 2) + ((180 - 175) ** 2) + ((2000 - 1500) ** 2)) ** 0.5\n",
        "print(f\"The Euclidean distance between Cliente1 and Cliente2 is {round(eucl_cl1_cl2, 1)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PA0HbDgZt1Vf"
      },
      "outputs": [],
      "source": [
        "df_clientes.T"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_value = 500.6"
      ],
      "metadata": {
        "id": "F5tk80JwYSiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-14T15:53:56.759415Z",
          "start_time": "2021-11-14T15:53:56.698577Z"
        },
        "id": "Zkd40oMnt1Vf"
      },
      "outputs": [],
      "source": [
        "distances = calculate_distances(X = df_clientes, index = index, distance_func = euclidean_distances)\n",
        "\n",
        "target_value = distances.iloc[1, 0]\n",
        "\n",
        "distances.style.\\\n",
        "applymap(lambda cell_value: format_cell_based_on_target_value(value = cell_value, target_value = target_value)).\\\n",
        "highlight_min(axis = 1, color = COLOR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwfOSpdGt1Vf"
      },
      "source": [
        "<a id = \"manhattan_bad\"></a>\n",
        "# --> Similitud basada en la distancia Manhattan (dataset sin estandarizar)\n",
        "[Volver al índice](#table_of_contents)\n",
        "\n",
        "La otra medida de distancia muy utilizada dentro del mundo del **Data Science** es la [distancia de Manhattan.](https://es.wikipedia.org/wiki/Geometr%C3%ADa_del_taxista)\n",
        "\n",
        "Al contrario que la distancia Euclídea (donde nos desplazamos por la diagonal), en la distancia de Manhattan nos desplazamos por los catetos del triángulo (no podemos atravesar un edificio en el Eixample de Barcelona). Para ello tomamos el **valor absoluto** de la diferencia entre los elementos de cada vector.\n",
        "\n",
        "La forma para calcular la distancia de Manhattan para N dimensiones viene dada por la siguiente fórmula:\n",
        "\n",
        "![Distancia_Manhattan_General](https://drive.google.com/uc?export=view&id=1v6V3MmdsubMwubFpOmN_zk6LQ81mMqiR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-14T15:53:56.775380Z",
          "start_time": "2021-11-14T15:53:56.761411Z"
        },
        "id": "XmLogAgjt1Vf"
      },
      "outputs": [],
      "source": [
        "manh_cl1_cl2 = (np.abs((18 - 30)) + np.abs((90 - 70)) + np.abs((180 - 175)) + np.abs((2000 - 1500)))\n",
        "print(f\"The Manhattan distance between Cliente1 and Cliente2 is {round(manh_cl1_cl2, 1)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yl4O0Zyrt1Vg"
      },
      "outputs": [],
      "source": [
        "df_clientes.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-14T15:53:56.837207Z",
          "start_time": "2021-11-14T15:53:56.777366Z"
        },
        "id": "zx1ebeNnt1Vg"
      },
      "outputs": [],
      "source": [
        "distances = calculate_distances(X = df_clientes, index = index, distance_func = manhattan_distances)\n",
        "\n",
        "target_value = distances.iloc[1, 0]\n",
        "\n",
        "distances.style.\\\n",
        "applymap(lambda cell_value: format_cell_based_on_target_value(value = cell_value, target_value = target_value)).\\\n",
        "highlight_min(axis = 1, color = COLOR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFdtVgrft1Vg"
      },
      "source": [
        "<a id = \"euclidea_bien\"></a>\n",
        "# --> Similitud basada en la distancia Euclídea (dataset estandarizado)\n",
        "[Volver al índice](#table_of_contents)\n",
        "\n",
        "Hemos podido ver en los ejemplos anteriores que al tener las 4 variables diferentes escalas, la variable de nómina se lleva todo el \"protagonismo\".\n",
        "\n",
        "La distancia entre el Cliente6 y Cliente4 es de 1.230 unidades. La diferencia de nómina que tienen es de 1.200.\n",
        "\n",
        "Claramente esto no es lo que me interesa, porque quiero tener en cuenta las 4 dimensiones descriptivas de mis clientes para determinar la similitud.\n",
        "\n",
        "Por este motivo, antes de calcular la distancia euclídea **tengo** que estandarizar mi dataset.\n",
        "\n",
        "Vamos a volver a calcular las distancias entre clientes, estandarizando previamente mi dataset con el StandardScaler.\n",
        "\n",
        "**RECORDAD:** StandardScaler no cambia la forma de la distribución, los valores escalados no están acotados pero sí que el resultado final es una dataset donde la media es cero y desviación típica es 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ivL8zSOt1Vg"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--imeC-9t1Vg"
      },
      "outputs": [],
      "source": [
        "scaled_df = scaler.fit_transform(df_clientes)\n",
        "scaled_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yfFWxxIt1Vg"
      },
      "outputs": [],
      "source": [
        "distances = calculate_distances(\n",
        "    X = scaled_df,\n",
        "    index = index,\n",
        "    distance_func = euclidean_distances\n",
        ")\n",
        "\n",
        "distances.style.highlight_min(axis = 1, color = COLOR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRKhB31Dt1Vg"
      },
      "source": [
        "<a id = \"manhattan_bien\"></a>\n",
        "# --> Similitud basada en la distancia Manhattan (dataset estandarizado)\n",
        "[Volver al índice](#table_of_contents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-14T15:53:57.089907Z",
          "start_time": "2021-11-14T15:53:57.061984Z"
        },
        "id": "GMOGf9yit1Vg"
      },
      "outputs": [],
      "source": [
        "distances_scaled = calculate_distances(X = scaled_df, index = index, distance_func = manhattan_distances)\n",
        "distances_scaled.style.highlight_min(axis = 1, color = COLOR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C74LSHCt1Vh"
      },
      "source": [
        "<a id = \"transformers\"></a>\n",
        "# --> Otras formas de procesar los datos\n",
        "[Volver al índice](#table_of_contents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri6ndTnxt1Vh"
      },
      "source": [
        "Otra forma de estandarizar nuestro dataset es utilizar el MinMaxScaler.\n",
        "\n",
        "**RECORDAD:** **MinMaxScaler** hace el valor más pequeño de cada columna sea 0 y el máximo 1, el resto de los valores están comprendidos entre este rango.\n",
        "\n",
        "[Más sobre el MinMaxScaler de sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-11-14T15:53:57.185652Z",
          "start_time": "2021-11-14T15:53:57.123818Z"
        },
        "id": "LuiwesRGt1Vh"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "\n",
        "df_clientes_max_scaled = scaler.fit_transform(df_clientes)\n",
        "\n",
        "df_clientes_max_scaled.columns = map(lambda name: \"MMS\" + name, df_clientes_max_scaled.columns)\n",
        "df_clientes_max_scaled = pd.concat([df_clientes_max_scaled, df_clientes], axis = 1)\n",
        "df_clientes_max_scaled\n",
        "\n",
        "df_clientes_max_scaled.style.highlight_min(axis = 0, color = COLOR).highlight_max(axis = 0, color = COLOR_MAX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb0IpTqEt1Vh"
      },
      "source": [
        "<a id = \"scaler_comparison\"></a>\n",
        "# Comparación de los Transformers\n",
        "[Volver al índice](#table_of_contents)\n",
        "\n",
        "En esta sección vamos a ver como afectan las diferentes transformaciones a la distribución y el rango de valores de un dataset.\n",
        "\n",
        "Recordamos que tenemos varias maneras de transformar los datos:\n",
        "\n",
        "*   ***StandardScaler :*** No cambia la forma de la distribución y los valores escalados no están acotados pero sí que el resultado final es una dataset donde la media es cero y desviación típica es 1\n",
        "*   ***MinMaxScaler :*** Los valores están acotados entre 0 - 1 y no cambia la forma\n",
        "*   ***Transformación logaritmica :*** En cambio aquí, sí que cambia la forma.  Esta transformación es particularmente útil cuando los datos tienen una distribución sesgada o presentan una gran variabilidad.\n",
        "*   ***Robust Scaler :*** No tiene en cuenta a los outliers ya que utiliza la mediana y el rango intercuartílico para escalar los datos basándose en estadísticas robustas."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = load_diabetes(as_frame = True, return_X_y = True)"
      ],
      "metadata": {
        "id": "wsFmS1vDd9IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = y.to_frame()"
      ],
      "metadata": {
        "id": "es1Ylkpid_f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "qpOFvV_lUtUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_standarized = pd.DataFrame(StandardScaler().fit_transform(y))\n",
        "y_min_max = pd.DataFrame(MinMaxScaler().fit_transform(y))\n",
        "y_log = pd.DataFrame(np.log(y))\n",
        "y_robust_scaled = pd.DataFrame(RobustScaler(quantile_range = (10.0, 90.0)).fit_transform(y))"
      ],
      "metadata": {
        "id": "_-KC81v6eVwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KIND = \"hist\"\n",
        "\n",
        "fig = plt.figure(figsize = (15, 5))\n",
        "axes1, axes2 = fig.subplots(2, 3)\n",
        "\n",
        "y.plot(kind = KIND, ax = axes1[0], title = \"Original Data\")\n",
        "y_standarized.plot(kind = KIND, ax = axes1[1], title = \"StandardScaler\")\n",
        "y_min_max.plot(kind = KIND, ax = axes1[2], title = \"MixMaxScaler\")\n",
        "y_log.plot(kind = KIND, ax = axes2[0], title = \"Log\")\n",
        "y_robust_scaled.plot(kind = KIND, ax = axes2[1], title = \"RobustScaler\")"
      ],
      "metadata": {
        "id": "X5dKO-gEeawp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WM1WgJzt1Vi"
      },
      "source": [
        "<a id = \"knn\"></a>\n",
        "# Aprendizaje Basado en Instancias\n",
        "[Volver al índice](#table_of_contents)\n",
        "\n",
        "Una vez que tenemos clara la idea de similitud y distancias, una cuestión muy interesante que puede surgir es:\n",
        "\n",
        "**¿Podemos aprovechar la distancia euclídea para resolver problemas no supervisados?**\n",
        "\n",
        "La respuesta es sí y uno de los algoritmos más usados para estas tareas se llama el [KNN (k-Nearest Neighbors)](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).\n",
        "\n",
        "Este algoritmo se llama \"lazy learners\" porque no existe un entrenamiento tal y como succede con otros modelos como LogisticRegression, Redes Neuronales u otros.\n",
        "\n",
        "En vez de esto, cada vez que me solicitan una predicción utilizo el KNN para \"buscar\" entre todas mis instancias y entre todas las dimensiones (variables) las ejemplos más próximos (utilizando la distancia euclídea) y hago mi predicción en función de la clase mayoritaría (problemas de clasificación) o bien voy a predecir la media en caso de problemas de regresión.\n",
        "\n",
        "![KNN](https://drive.google.com/uc?export=view&id=1_lhK7u-MwqIU-cGg_caeu17HbUQL-WUX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnOqCrklt1Vi"
      },
      "outputs": [],
      "source": [
        "# cargamos nuestro dataset\n",
        "X_train = pd.read_csv(\"/content/drive/MyDrive/Nuclio_No_Supervisado/unsupervised_learning/data/pd_sklearn_data.csv\")\n",
        "\n",
        "# ponemos en el índice PassengerId\n",
        "X_train.set_index(\"PassengerId\", inplace = True)\n",
        "\n",
        "# nos quedamos con las columnas numéricas\n",
        "X_train = X_train[[\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Sex\"]]\n",
        "\n",
        "# transformamos columna Sex en booleana\n",
        "X_train[\"Sex\"] = X_train[\"Sex\"].apply(lambda sex: 1 if sex == \"female\" else 0)\n",
        "\n",
        "# creamos un indicador de que pasajero tenían nulos\n",
        "X_train[\"Null_Age\"] = X_train[\"Age\"].isnull()\n",
        "\n",
        "# guardamos el indicador de nulos en edad\n",
        "null_age = X_train[[\"Null_Age\"]]\n",
        "\n",
        "# nos quedamos con las númericas\n",
        "X_train.drop(columns = \"Null_Age\", inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM0tuN5Ft1Vi"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "mWpNBFLDkIV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1bJ0OB-t1Vi"
      },
      "outputs": [],
      "source": [
        "# instanciamos nuestro KNNImputer\n",
        "knn_imputer = KNNImputer(n_neighbors = 7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baqplyYIt1Vi"
      },
      "outputs": [],
      "source": [
        "# Imputamos los nulos en base a 7 vecinos más cercanos\n",
        "X_train_imputed = knn_imputer.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5W2iYBfAt1Vj"
      },
      "outputs": [],
      "source": [
        "# mergemos el flag de nulos y miramos que valores hemos imputado\n",
        "X_train_imputed = X_train_imputed.merge(null_age, how = \"left\", left_index = True, right_index = True)\n",
        "\n",
        "# filtramos algunos nulos a modo de ejemplos\n",
        "X_train_imputed[X_train_imputed[\"Null_Age\"] == True].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBonwpdCt1Vj"
      },
      "outputs": [],
      "source": [
        "X_train[X_train[\"Age\"].isnull()].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJn381lOt1Vj"
      },
      "source": [
        "\n",
        "El algoritmo de KNN es extremadamente potente y versátil, pero tiene 2 carencias fundamentales:\n",
        "1. No es muy escalable porque para cada nueva predicción tengo que cargar toda la base de datos. Esto implica que para cada nueva predicción, tengo que calcular todas las distancias y ver cuales son estos vecinos más cercanos y en función de estos hacer el predict. Esto no succede con una Red Neuronal porque durante el entrenamiento aprendió del dataset y puede hacer el predict a partir de los paramétros inferidos en el train.\n",
        "\n",
        "\n",
        "2. Un segundo problema que se presenta es que KNN necesita mucho espacio en memoria. Se tiene que cargar el dataset en memoria para hacer la búsqueda y en determinados contextos esto puede suponer un problema.\n",
        "\n",
        "Por los dos motivos antes expuestos se recomienda usar el KNN para datasets pequeños."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaJkAF4Ct1Vj"
      },
      "source": [
        "<a id = \"conclusión\"></a>\n",
        "# Conclusión\n",
        "[Volver al índice](#table_of_contents)\n",
        "\n",
        "Las principales conclusiones que podemos extraer de este notebook son:\n",
        "\n",
        "1. La distancia Euclídea o la de Manhattan son dos formas de calcular la proximidad entre dos vectores en un espacio N dimensional. Esta distancia se puede utilizar en determinados contextos como \"proxy\" a similitud entre clientes. Algunos de los algoritmos que usan estas distancias son KMeans, KNN o DBScan.\n",
        "\n",
        "\n",
        "2. Si nuestro dataset tiene variables que presentan diferentes escalas, es **fundamental** estandarizar antes estos valores (utilizando el StandardScaler o bien el MinMaxScaler) para que el aporte de cada atributo sea ~ el mismo.\n",
        "\n",
        "\n",
        "3. Existen algoritmos que pueden hacer uso de la idea de \"proximidad\" para hacer muy buenas predicciones para datasets pequeños y medianos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RI-rQ4Kt1Vj"
      },
      "source": [
        "<a id='referencias'></a>\n",
        "# Referencias y lecturas recomendables\n",
        "[Volver al índice](#table_of_contents)<br>\n",
        "\n",
        "A continuación dejamos algunos links útiles para profundizar en algunos de los conceptos que hemos visto en el notebook:\n",
        "\n",
        "[Curse of Dimensionality](https://towardsdatascience.com/the-curse-of-dimensionality-50dc6e49aa1e)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env_main",
      "language": "python",
      "name": "env_main"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}